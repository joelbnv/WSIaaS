# üõ†Ô∏è Web Scraping Intelligence as a Service - Scraper Backend TFG - ETL para tiendas eCommerce

Este proyecto realiza un proceso ETL (Extracci√≥n, Transformaci√≥n y Carga) para diferentes plataformas de tiendas en l√≠nea como **Shopify**, **PrestaShop**, **BigCommerce**, **WooCommerce** y **Wix**. Usa contenedores Docker para facilitar la ejecuci√≥n y asegurar la portabilidad del entorno

## üöÄ Requisitos

- [Docker](https://docs.docker.com/get-docker/)
- [Docker Compose](https://docs.docker.com/compose/install/)
- Acceso a Internet (para el scraping)

---

## üê≥ Construcci√≥n de la imagen

Antes de ejecutar cualquier comando, aseg√∫rate de construir las im√°genes de Docker:

```bash
docker compose build
```

## Arranque de servicios de base de datos y proxy

Si deseas levantar los servicios auxiliares (MySQL, PostgreSQL y el proxy TinyProxy), ejecuta:

```bash
docker compose up -d mysql postgres tinyproxy
```

Puedes detenerlos con:

```bash
docker compose down
```

# ‚öôÔ∏è Ejecuci√≥n del proceso ETL

## Salida en formato CSV (recomendado para pruebas r√°pidas):

```bash
docker compose run --rm app \
  --url tirachinas.shop \
  --vendor shopify \
  --destination-format csv \
  --destination-path result_files/output.csv
```

## Formato SQLite

```bash
docker compose run --rm app \
  --url tirachinas.shop \
  --vendor shopify \
  --destination-format sqlite \
  --db-path sqlite_dbs/mi_etl.db
```

## Formato MySQL

```bash
docker compose run --rm app \
  --url tirachinas.shop \
  --vendor shopify \
  --destination-format mysql \
  --db-config '{"host": "mysql", "port": 3306, "user": "root", "password": "root", "database": "etl"}'

```

## Formato PostgreSQL

```bash
docker compose run --rm app \
  --url tirachinas.shop \
  --vendor shopify \
  --destination-format postgres \
  --db-config '{"host": "postgres", "port": 5432, "user": "postgres", "password": "postgres", "database": "etl"}'
```


# üß™ Argumentos disponibles

| Argumento               | Obligatorio | Valores posibles                                                                                      | Descripci√≥n                                                                                          |
|-------------------------|-------------|--------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------|
| `--url`                 | ‚úÖ S√≠        | Cadena (sin `https://`)                                                                               | Dominio de la tienda a scrapear (ejemplo: `tirachinas.shop`)                                       |
| `--vendor`              | ‚úÖ S√≠        | `shopify`, `prestashop`, `bigcommerce`, `woocommerce`, `wix`                                          | Plataforma (vendor) de la tienda                                                                    |
| `--destination-path`    | ‚ùå No        | Carpeta o archivo                                                                                      | Carpeta o archivo donde se guardar√°n los resultados transformados y cargados                        |
| `--destination-format`  | ‚ùå No        | `csv`, `jsonl`, `excel`, `parquet`, `sqlite`, `mysql`, `postgres`                                     | Formato de salida para los datos cargados (por defecto: `csv`)                                      |
| `--db-type`             | ‚ùå No        | `sqlite`, `mysql`, `postgresql`                                                                        | Tipo de base de datos (opcional, puede omitirse si se define `--destination-format`)                |
| `--db-config`           | ‚ùå S√≠*       | JSON en string (ej: `{"host": "mysql", "port": 3306, "user": "...", ...}`)                            | Requerido si el destino es MySQL o PostgreSQL                                                        |
| `--db-path`             | ‚ùå S√≠*       | Ruta a archivo `.db`                                                                                   | Requerido si el destino es SQLite                                                                    |


- Los argumentos marcados con ‚ùå S√≠* son sobligatorios dependiendo del valor de `--destination-format`.


# üîç Debugging

Para entrar al contenedor y ejecutar comandos manualmente:

```bash
docker compose run --rm --entrypoint bash app
```

Desde all√≠, puedes ejecutar el script manualmente:

```bash
python main.py --url tirachinas.shop --vendor shopify --destination-format csv --destination-path result_files
```

# üìÇ Estructura de carpetas

- result_files/: Archivos generados por el proceso ETL.
- sqlite_dbs/: Archivos .db si usas SQLite.
- scraper/, transformer/, loader/: M√≥dulos de la pipeline ETL


# Notas

- El contenedor tinyproxy se utiliza para anonimizar el scraping v√≠a proxy (puerto 8888).
- El par√°metro PROXY_HOST se configura autom√°ticamente en el entorno de Docker Compose (tinyproxy).
- Para pruebas en local sin Docker, puedes ejecutar el script con el entorno virtual:

    Si se usa un entorno virtual en Windows, una vez satisfechas las dependencias especificadas en **requirements.txt**

    - Crear entorno virtual (desde la carpeta del proyecto):

    ```bash
    python3 -m venv .venv
    ```

    - Activar entorno virtual (desde la carpeta del proyecto):

    ```bash
    & .\.venv\Scripts\activate.ps1
    ```
    
    - Statisfacer dependencias:

    ```bash
    pip install -r requirements.txt
    ```

    - Ejecutar proceso ETL:

    ```bash
    python main.py --url tirachinas.shop --vendor shopify --destination-format csv --destination-path result_files
    ```

# Licencia

Este proyecto se desarrolla como parte del Trabajo de Fin de Grado.